{
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pickle"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-04T02:43:10.760355Z",
     "start_time": "2024-06-04T02:43:10.755300Z"
    }
   },
   "id": "8396c7407557ba33",
   "execution_count": 35,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# 激活函数及其导数\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return np.where(x > 0, 1, 0)\n",
    "\n",
    "\n",
    "# 多分类\n",
    "def softmax(x):\n",
    "    exps = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    return exps / np.sum(exps, axis=1, keepdims=True)\n",
    "\n",
    "# 损失函数及其导数\n",
    "def cross_entropy_loss(y_true, y_pred):\n",
    "    loss = -np.mean(np.sum(y_true * np.log(y_pred + 1e-15), axis=1))\n",
    "    return loss\n",
    "\n",
    "\n",
    "def cross_entropy_loss_derivative(y_true, y_pred):\n",
    "    return y_pred - y_true\n",
    "\n",
    "\n",
    "# MLP类定义\n",
    "class MLPClassifier:\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, output_size, learning_rate=0.01, batch_size=32):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size1 = hidden_size1\n",
    "        self.hidden_size2 = hidden_size2\n",
    "        self.output_size = output_size\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate=learning_rate\n",
    "\n",
    "        # 初始化权重和偏置\n",
    "        self.W1 = np.random.randn(input_size, hidden_size1) * 0.01\n",
    "        self.b1 = np.zeros((1, hidden_size1))\n",
    "        self.W2 = np.random.randn(hidden_size1, hidden_size2) * 0.01\n",
    "        self.b2 = np.zeros((1, hidden_size2))\n",
    "        self.W3 = np.random.randn(hidden_size2, output_size) * 0.01\n",
    "        self.b3 = np.zeros((1, output_size))\n",
    "\n",
    "    def forward(self, X):\n",
    "        # 前向传播\n",
    "        self.z1 = np.dot(X, self.W1) + self.b1\n",
    "        self.a1 = relu(self.z1)\n",
    "        self.z2 = np.dot(self.a1, self.W2) + self.b2\n",
    "        self.a2 = relu(self.z2)\n",
    "        self.z3 = np.dot(self.a2, self.W3) + self.b3\n",
    "        self.a3 = softmax(self.z3)  # 输出层使用Softmax激活函数\n",
    "        return self.a3\n",
    "\n",
    "    def backward(self, X, y):\n",
    "        # 反向传播\n",
    "        m = X.shape[0]  # 使用输入样本数\n",
    "        # print(f\"m:{m}\")\n",
    "\n",
    "        # 计算输出层的梯度\n",
    "        d_loss_a3 = cross_entropy_loss_derivative(y, self.a3)\n",
    "\n",
    "        # 计算第三层的梯度\n",
    "        d_loss_W3 = np.dot(self.a2.T, d_loss_a3) / m\n",
    "        d_loss_b3 = np.sum(d_loss_a3, axis=0, keepdims=True) / m\n",
    "\n",
    "        # 计算第二层的梯度\n",
    "        d_loss_a2 = np.dot(d_loss_a3, self.W3.T)\n",
    "        d_loss_z2 = d_loss_a2 * relu_derivative(self.z2)\n",
    "        d_loss_W2 = np.dot(self.a1.T, d_loss_z2) / m\n",
    "        d_loss_b2 = np.sum(d_loss_z2, axis=0, keepdims=True) / m\n",
    "\n",
    "        # 计算第一层的梯度\n",
    "        d_loss_a1 = np.dot(d_loss_z2, self.W2.T)\n",
    "        d_loss_z1 = d_loss_a1 * relu_derivative(self.z1)\n",
    "        d_loss_W1 = np.dot(X.T, d_loss_z1) / m\n",
    "        d_loss_b1 = np.sum(d_loss_z1, axis=0, keepdims=True) / m\n",
    "\n",
    "        # 更新权重和偏置\n",
    "        self.W3 -= self.learning_rate * d_loss_W3\n",
    "        self.b3 -= self.learning_rate * d_loss_b3\n",
    "        self.W2 -= self.learning_rate * d_loss_W2\n",
    "        self.b2 -= self.learning_rate * d_loss_b2\n",
    "        self.W1 -= self.learning_rate * d_loss_W1\n",
    "        self.b1 -= self.learning_rate * d_loss_b1\n",
    "\n",
    "    def train(self, X, y, epochs):\n",
    "        n_samples = X.shape[0]\n",
    "        n_batches = n_samples // self.batch_size\n",
    "        for epoch in range(epochs):\n",
    "            # 随机打乱数据集\n",
    "            shuffled_indices = np.random.permutation(n_samples)\n",
    "            X_shuffled = X[shuffled_indices]\n",
    "            y_shuffled = y[shuffled_indices]\n",
    "            batch_losses = []  # 存储每个批次的损失值\n",
    "            # 小批量梯度下降\n",
    "            for batch in range(n_batches):\n",
    "                start = batch * self.batch_size\n",
    "                end = start + self.batch_size\n",
    "                X_batch = X_shuffled[start:end]\n",
    "                y_batch = y_shuffled[start:end]\n",
    "                output = self.forward(X_batch)\n",
    "                loss = cross_entropy_loss(y_batch, output)\n",
    "                batch_losses.append(loss)\n",
    "                self.backward(X_batch, y_batch)\n",
    "            # 处理剩余样本\n",
    "            if n_samples % self.batch_size != 0:\n",
    "                start = n_batches * self.batch_size\n",
    "                X_batch = X_shuffled[start:]\n",
    "                y_batch = y_shuffled[start:]\n",
    "                output = self.forward(X_batch)\n",
    "                loss = cross_entropy_loss(y_batch, output)\n",
    "                batch_losses.append(loss)\n",
    "                self.backward(X_batch, y_batch, self.learning_rate)\n",
    "            # 打印每个epoch的平均损失\n",
    "            epoch_loss = sum(batch_losses) / len(batch_losses)\n",
    "            print(f'Epoch {epoch+1}/{epochs}, Average Loss: {epoch_loss:.4f}')\n",
    "\n",
    "    def predict(self, X):\n",
    "        output = self.forward(X)\n",
    "        return np.argmax(output, axis=1)\n",
    "\n",
    "    def accuracy(self, X, y):\n",
    "        y_predicted=self.forward(X)\n",
    "        predictions = np.argmax(y_predicted, axis=1)\n",
    "        labels = np.argmax(y, axis=1)\n",
    "        return np.mean(predictions == labels)\n",
    "    \n",
    "    \n",
    "    \n",
    "    def save_weights(self, file_path):\n",
    "        weights = {\n",
    "            'W1': self.W1,\n",
    "            'b1': self.b1,\n",
    "            'W2': self.W2,\n",
    "            'b2': self.b2,\n",
    "            'W3': self.W3,\n",
    "            'b3': self.b3\n",
    "        }\n",
    "        with open(file_path, 'wb') as file:\n",
    "            pickle.dump(weights, file)\n",
    "\n",
    "    def load_weights(self, file_path):\n",
    "        with open(file_path, 'rb') as file:\n",
    "            weights = pickle.load(file)\n",
    "            self.W1 = weights['W1']\n",
    "            self.b1 = weights['b1']\n",
    "            self.W2 = weights['W2']\n",
    "            self.b2 = weights['b2']\n",
    "            self.W3 = weights['W3']\n",
    "            self.b3 = weights['b3']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-04T02:43:10.790253Z",
     "start_time": "2024-06-04T02:43:10.762275Z"
    }
   },
   "id": "ddb587b476d2838d",
   "execution_count": 36,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def load_mnist_data(train_path):\n",
    "    # 加载CSV文件\n",
    "    train_data = pd.read_csv(train_path).values\n",
    "\n",
    "    # # 打印数据集基本信息\n",
    "    # print(f\"Number of rows: {train_data.shape[0]}, Number of columns: {train_data.shape[1]}\")\n",
    "\n",
    "    # 提取特征和标签\n",
    "    X_train = train_data[:, 1:]\n",
    "    y_train = train_data[:, 0]\n",
    "\n",
    "    # # 打印特征和标签的维度\n",
    "    # print(f\"Shape of X_train: {X_train.shape}\")\n",
    "    # print(f\"Shape of y_train: {y_train.shape}\")\n",
    "    # \n",
    "    # # 打印部分数据以检查\n",
    "    # print(\"First 5 rows of X_train:\")\n",
    "    # print(X_train[:5])\n",
    "    # print(\"First 5 labels of y_train:\")\n",
    "    # print(y_train[:5])\n",
    "\n",
    "    # 归一化像素值到 [0, 1]\n",
    "    X_train = X_train / 255.0\n",
    "\n",
    "    # 将标签转换为独热编码\n",
    "    y_train = np.eye(10)[y_train.astype(int)]\n",
    "\n",
    "    # # 打印转换后的独热编码标签\n",
    "    # print(\"First 5 rows of one-hot encoded y_train:\")\n",
    "    # print(y_train[:5])\n",
    "\n",
    "    return X_train, y_train"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-04T02:43:10.801145Z",
     "start_time": "2024-06-04T02:43:10.792175Z"
    }
   },
   "id": "e55a30ad94c995e",
   "execution_count": 37,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "X_train,y_train=load_mnist_data('./data/mnist_train.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-04T02:43:15.189935Z",
     "start_time": "2024-06-04T02:43:10.803139Z"
    }
   },
   "id": "3d9cd80324e55dd6",
   "execution_count": 38,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# 创建MLP模型\n",
    "mlp = MLPClassifier(input_size=784, hidden_size1=128, hidden_size2=64, output_size=10,learning_rate=0.005)\n",
    "#\n",
    "# # 训练MLP模型\n",
    "mlp.train(X_train, y_train, epochs=100)\n",
    "#\n",
    "# 保存训练后的权重\n",
    "mlp.save_weights('./weight/mlpclassifier_weights.pkl')\n",
    "# 评估模型\n",
    "train_accuracy = mlp.accuracy(X_train, y_train)\n",
    "\n",
    "print(f'Train Accuracy: {train_accuracy * 100:.2f}%')\n"
   ],
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-04T03:23:55.027454Z",
     "start_time": "2024-06-04T03:15:50.015277Z"
    }
   },
   "id": "initial_id",
   "execution_count": 40,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Average Loss: 2.3011\n",
      "Epoch 2/100, Average Loss: 2.2592\n",
      "Epoch 3/100, Average Loss: 1.0944\n",
      "Epoch 4/100, Average Loss: 0.6186\n",
      "Epoch 5/100, Average Loss: 0.5011\n",
      "Epoch 6/100, Average Loss: 0.3955\n",
      "Epoch 7/100, Average Loss: 0.3346\n",
      "Epoch 8/100, Average Loss: 0.2920\n",
      "Epoch 9/100, Average Loss: 0.2576\n",
      "Epoch 10/100, Average Loss: 0.2296\n",
      "Epoch 11/100, Average Loss: 0.2060\n",
      "Epoch 12/100, Average Loss: 0.1873\n",
      "Epoch 13/100, Average Loss: 0.1712\n",
      "Epoch 14/100, Average Loss: 0.1586\n",
      "Epoch 15/100, Average Loss: 0.1477\n",
      "Epoch 16/100, Average Loss: 0.1380\n",
      "Epoch 17/100, Average Loss: 0.1298\n",
      "Epoch 18/100, Average Loss: 0.1219\n",
      "Epoch 19/100, Average Loss: 0.1149\n",
      "Epoch 20/100, Average Loss: 0.1085\n",
      "Epoch 21/100, Average Loss: 0.1025\n",
      "Epoch 22/100, Average Loss: 0.0971\n",
      "Epoch 23/100, Average Loss: 0.0919\n",
      "Epoch 24/100, Average Loss: 0.0877\n",
      "Epoch 25/100, Average Loss: 0.0829\n",
      "Epoch 26/100, Average Loss: 0.0785\n",
      "Epoch 27/100, Average Loss: 0.0746\n",
      "Epoch 28/100, Average Loss: 0.0712\n",
      "Epoch 29/100, Average Loss: 0.0674\n",
      "Epoch 30/100, Average Loss: 0.0644\n",
      "Epoch 31/100, Average Loss: 0.0614\n",
      "Epoch 32/100, Average Loss: 0.0587\n",
      "Epoch 33/100, Average Loss: 0.0556\n",
      "Epoch 34/100, Average Loss: 0.0534\n",
      "Epoch 35/100, Average Loss: 0.0511\n",
      "Epoch 36/100, Average Loss: 0.0488\n",
      "Epoch 37/100, Average Loss: 0.0466\n",
      "Epoch 38/100, Average Loss: 0.0446\n",
      "Epoch 39/100, Average Loss: 0.0426\n",
      "Epoch 40/100, Average Loss: 0.0409\n",
      "Epoch 41/100, Average Loss: 0.0391\n",
      "Epoch 42/100, Average Loss: 0.0373\n",
      "Epoch 43/100, Average Loss: 0.0358\n",
      "Epoch 44/100, Average Loss: 0.0346\n",
      "Epoch 45/100, Average Loss: 0.0328\n",
      "Epoch 46/100, Average Loss: 0.0315\n",
      "Epoch 47/100, Average Loss: 0.0301\n",
      "Epoch 48/100, Average Loss: 0.0288\n",
      "Epoch 49/100, Average Loss: 0.0277\n",
      "Epoch 50/100, Average Loss: 0.0264\n",
      "Epoch 51/100, Average Loss: 0.0253\n",
      "Epoch 52/100, Average Loss: 0.0243\n",
      "Epoch 53/100, Average Loss: 0.0234\n",
      "Epoch 54/100, Average Loss: 0.0221\n",
      "Epoch 55/100, Average Loss: 0.0214\n",
      "Epoch 56/100, Average Loss: 0.0205\n",
      "Epoch 57/100, Average Loss: 0.0199\n",
      "Epoch 58/100, Average Loss: 0.0188\n",
      "Epoch 59/100, Average Loss: 0.0182\n",
      "Epoch 60/100, Average Loss: 0.0175\n",
      "Epoch 61/100, Average Loss: 0.0168\n",
      "Epoch 62/100, Average Loss: 0.0160\n",
      "Epoch 63/100, Average Loss: 0.0154\n",
      "Epoch 64/100, Average Loss: 0.0148\n",
      "Epoch 65/100, Average Loss: 0.0141\n",
      "Epoch 66/100, Average Loss: 0.0136\n",
      "Epoch 67/100, Average Loss: 0.0130\n",
      "Epoch 68/100, Average Loss: 0.0125\n",
      "Epoch 69/100, Average Loss: 0.0121\n",
      "Epoch 70/100, Average Loss: 0.0114\n",
      "Epoch 71/100, Average Loss: 0.0112\n",
      "Epoch 72/100, Average Loss: 0.0106\n",
      "Epoch 73/100, Average Loss: 0.0102\n",
      "Epoch 74/100, Average Loss: 0.0099\n",
      "Epoch 75/100, Average Loss: 0.0095\n",
      "Epoch 76/100, Average Loss: 0.0091\n",
      "Epoch 77/100, Average Loss: 0.0087\n",
      "Epoch 78/100, Average Loss: 0.0085\n",
      "Epoch 79/100, Average Loss: 0.0080\n",
      "Epoch 80/100, Average Loss: 0.0079\n",
      "Epoch 81/100, Average Loss: 0.0075\n",
      "Epoch 82/100, Average Loss: 0.0073\n",
      "Epoch 83/100, Average Loss: 0.0070\n",
      "Epoch 84/100, Average Loss: 0.0068\n",
      "Epoch 85/100, Average Loss: 0.0064\n",
      "Epoch 86/100, Average Loss: 0.0063\n",
      "Epoch 87/100, Average Loss: 0.0061\n",
      "Epoch 88/100, Average Loss: 0.0059\n",
      "Epoch 89/100, Average Loss: 0.0057\n",
      "Epoch 90/100, Average Loss: 0.0055\n",
      "Epoch 91/100, Average Loss: 0.0054\n",
      "Epoch 92/100, Average Loss: 0.0051\n",
      "Epoch 93/100, Average Loss: 0.0050\n",
      "Epoch 94/100, Average Loss: 0.0048\n",
      "Epoch 95/100, Average Loss: 0.0047\n",
      "Epoch 96/100, Average Loss: 0.0045\n",
      "Epoch 97/100, Average Loss: 0.0044\n",
      "Epoch 98/100, Average Loss: 0.0043\n",
      "Epoch 99/100, Average Loss: 0.0041\n",
      "Epoch 100/100, Average Loss: 0.0040\n",
      "Train Accuracy: 99.99%\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "X_test,y_test=load_mnist_data('./data/mnist_test.csv')\n",
    "test_accuracy = mlp.accuracy(X_test, y_test)\n",
    "print(f'Test Accuracy: {test_accuracy * 100:.2f}%')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-04T03:23:57.974382Z",
     "start_time": "2024-06-04T03:23:55.040400Z"
    }
   },
   "id": "532fab2647fdbda8",
   "execution_count": 41,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 97.52%\n"
     ]
    }
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
