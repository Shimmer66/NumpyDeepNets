#### 常用的激活函数：

1. **Sigmoid**

   ![img](https://mse.xauat.edu.cn/__local/3/A3/28/04AA537B1B988B8C787C1A3FB1D_43A9C422_11D4D.png)

   ![img](https://mse.xauat.edu.cn/__local/7/C0/DB/CA6043A0088D9586B127D0002DD_044BC3A1_2810.png)

   - Sigmoid 函数的输出范围是 0 到 1。由于输出值限定在 0 到 1，因此它对每个神经元的输出进行了归一化；
   - 用于将预测概率作为输出的模型。由于概率的取值范围是 0 到 1，因此 Sigmoid 函数非常合适；
   - 梯度平滑，避免「跳跃」的输出值；
   - 函数是可微的。这意味着可以找到任意两个点的 sigmoid 曲线的斜率；
   - 明确的预测，即非常接近 1 或 0。

   Sigmoid 激活函数有哪些缺点？

   - 倾向于梯度消失；
   - 函数输出不是以 0 为中心的，这会降低权重更新的效率；
   - Sigmoid 函数执行指数运算，计算机运行得较慢。

2. **Tanh**:

   ![img](https://mse.xauat.edu.cn/__local/6/B1/86/B83997B0B9421018BFD2F629B76_B240D58B_2E3D.png)

   ![img](https://mse.xauat.edu.cn/__local/D/72/B5/75CC9658AC6F284FC1942D72247_131DE4AA_2349.png)

   ![img](https://mse.xauat.edu.cn/__local/6/0E/E9/0AC71EE220468A2E4A30B39C6D7_95C5D2F1_16C99.png)

   - 首先，当输入较大或较小时，输出几乎是平滑的并且梯度较小，这不利于权重更新。二者的区别在于输出间隔，tanh 的输出间隔为 1，并且整个函数以 0 为中心，比 sigmoid 函数更好；
   - 在 tanh 图中，负输入将被强映射为负，而零输入被映射为接近零。
   - 在一般的二元分类问题中，tanh 函数用于隐藏层，而 sigmoid 函数用于输出层，但这并不是固定的，需要根据特定问题进行调整。

3. **ReLU (Rectified Linear Unit)**: 

   ![img](https://mse.xauat.edu.cn/__local/5/C3/61/F9760C84FF4579F86ED46D737CB_EEC4283B_439F.png)

   ![img](https://mse.xauat.edu.cn/__local/E/68/29/14746F21F65793740AF99C1B061_4B6FA929_290D.png)

   ReLU 函数是深度学习中较为流行的一种激活函数，相比于 sigmoid 函数和 tanh 函数，它具有如下优点：

   - 当输入为正时，不存在梯度饱和问题。
   - 计算速度快得多。ReLU 函数中只存在线性关系，因此它的计算速度比 sigmoid 和 tanh 更快。

   当然，它也有缺点：

   1. Dead ReLU 问题。当输入为负时，ReLU 完全失效，在正向传播过程中，这不是问题。有些区域很敏感，有些则不敏感。但是在反向传播过程中，如果输入负数，则梯度将完全为零，sigmoid 函数和 tanh 函数也具有相同的问题；
   2. 我们发现 ReLU 函数的输出为 0 或正数，这意味着 ReLU 函数不是以 0 为中心的函数。
   
4. **Leaky ReLU**

   ![img](https://mse.xauat.edu.cn/__local/A/2A/93/BD13C3B177644C3BF69A0A3E084_B2896AE5_EF99.png)

   ![img](https://mse.xauat.edu.cn/__local/5/7B/80/C821EA6D0FC7B8235F4266DBC5B_9E017449_14882.png)

   1. Leaky ReLU 通过把 x 的非常小的线性分量给予负输入（0.01x）来调整负值的零梯度（zero gradients）问题；
   2. leak 有助于扩大 ReLU 函数的范围，通常 a 的值为 0.01 左右；
   3. Leaky ReLU 的函数范围是（负无穷到正无穷）。

   **注意：**从理论上讲，Leaky ReLU 具有 ReLU 的所有优点，而且 Dead ReLU 不会有任何问题，但在实际操作中，尚未完全证明 Leaky ReLU 总是比 ReLU 更好。


5. **ELU (Exponential Linear Unit)**: 

   ![img](https://mse.xauat.edu.cn/__local/B/5C/61/550F72983FCAF04D45B330F4DB0_EFA34A18_2D92.png)
   
   ![img](https://mse.xauat.edu.cn/__local/9/18/4E/5A446503954BE8EACC4451576F1_792EE7D6_7DBD.png)
   
   ELU 的提出也解决了 ReLU 的问题。与 ReLU 相比，ELU 有负值，这会使激活的平均值接近零。均值激活接近于零可以使学习更快，因为它们使梯度更接近自然梯度。
   
   显然，ELU 具有 ReLU 的所有优点，并且：
   
   - 没有 Dead ReLU 问题，输出的平均值接近 0，以 0 为中心；
   - ELU 通过减少偏置偏移的影响，使正常梯度更接近于单位自然梯度，从而使均值向零加速学习；
   - ELU 在较小的输入下会饱和至负值，从而减少前向传播的变异和信息。
   
   一个小问题是它的计算强度更高。与 Leaky ReLU 类似，尽管理论上比 ReLU 要好，但目前在实践中没有充分的证据表明 ELU 总是比 ReLU 好。

6. **Softmax **

   ![img](https://mse.xauat.edu.cn/__local/1/75/DB/444D7A6E04F4B15492F7C19C9A1_5162184A_91B7.png)

   Softmax 是用于多类分类问题的激活函数，在多类分类问题中，超过两个类标签则需要类成员关系。对于长度为 K 的任意实向量，Softmax 可以将其压缩为长度为 K，值在（0，1）范围内，并且向量中元素的总和为 1 的实向量。

   ![img](https://mse.xauat.edu.cn/__local/0/34/83/001AC5AEC3AEF69030E04871D39_48A56E3D_14EA1.png)

   Softmax 与正常的 max 函数不同：max 函数仅输出最大值，但 Softmax 确保较小的值具有较小的概率，并且不会直接丢弃。我们可以认为它是 argmax 函数的概率版本或「soft」版本。

   Softmax 函数的分母结合了原始输出值的所有因子，这意味着 Softmax 函数获得的各种概率彼此相关。

   Softmax 激活函数的主要缺点是：

   1. 在零点不可微；
   2. 负输入的梯度为零，这意味着对于该区域的激活，权重不会在反向传播期间更新，因此会产生永不激活的死亡神经元

#### 常用的损失函数：

1. **回归损失**

   - **均方误差**：

     均方误差（MSE）度量的是预测值和实际观测值间差的平方的均值。它只考虑误差的平均大小，不考虑其方向。但由于经过平方，与真实值偏离较多的预测值会比偏离较少的预测值受到更为严重的惩罚。再加上 MSE 的数学特性很好，这使得计算梯度变得更容易。

   ![img](https://image.jiqizhixin.com/uploads/editor/041422c3-5343-4df6-9156-5a16d35b6155/1536733519963.png)

   - **平均绝对误差/L1 损失**：

     平均绝对误差（MAE）度量的是预测值和实际观测值之间绝对差之和的平均值。和 MSE 一样，这种度量方法也是在不考虑方向的情况下衡量误差大小。但和 MSE 的不同之处在于，MAE 需要像线性规划这样更复杂的工具来计算梯度。此外，MAE 对异常值更加稳健，因为它不使用平方。

     ![img](https://image.jiqizhixin.com/uploads/editor/0dbb3177-a28d-47a6-ba49-18147b4226f8/1536733519513.png)

   

2. **分类损失**

   - **Hinge Loss/多分类 SVM 损失**：

     在一定的安全间隔内（通常是 1），正确类别的分数应高于所有错误类别的分数之和。因此 hinge loss 常用于最大间隔分类（maximum-margin classification），最常用的是支持向量机。尽管不可微，但它是一个凸函数，因此可以轻而易举地使用机器学习领域中常用的凸优化器。

   ![img](https://image.jiqizhixin.com/uploads/editor/177ac671-9e93-43ba-9cd7-9b0748298882/1536733519318.png)
   
   - **交叉熵损失/负对数似然**：
   
     当实际标签为 1(y(i)=1) 时，函数的后半部分消失，而当实际标签是为 0(y(i=0)) 时，函数的前半部分消失。简言之，我们只是把对真实值类别的实际预测概率的对数相乘。还有重要的一点是，交叉熵损失会重重惩罚那些置信度高但是错误的预测值。
   
     ![img](https://image.jiqizhixin.com/uploads/editor/03faef70-8369-492c-82a4-3abe951d52a6/1536733519906.png)
   
   - **0-1损失函数**：
   
     0-1损失函数直接对应分类判断错误的个数，但是它是一个非凸函数，不太实用， 感知机就是用的这种损失函数。但是相等这个条件太过严格，因此可以放宽条件，即满足 ∣ Y − f ( x ) ∣ < T |Y-f(x)| < T∣Y−f(x)∣<T 时认为相等。
   
    ![img](https://images2018.cnblogs.com/blog/1192699/201806/1192699-20180623142507596-329289242.png)
   
   - **绝对值损失函数**：
   
     ![img](https://images2018.cnblogs.com/blog/1192699/201806/1192699-20180623142527347-1339325160.png)

#### 常用的正则化方法：

正则化(Regularization)指的是通过引入噪声或限制模型的复杂度，降低模型对输入或者参数的敏感性，避免过拟合，提高模型的泛化能力。常用的正则化方法包括约束目标函数(等价于约束模型参数)、约束网络结构、约束优化过程。

约束目标函数：在目标函数中增加模型参数的正则化项，包括L2正则化, L1正则化, 弹性网络正则化, L0正则化, 谱正则化, 自正交性正则化, WEISSI正则化, 梯度惩罚
约束网络结构：在网络结构中添加噪声，包括随机深度, Dropout及其系列方法,
约束优化过程：在优化过程中施加额外步骤，包括数据增强, 梯度裁剪, Early Stop, 标签平滑, 变分信息瓶颈, 虚拟对抗训练, Flooding

1. **L2正则化**

   L2正则化通过约束参数的L2范数（L2-norm）减小过拟合。

   ![image-20240526113805362](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20240526113805362.png)

2.  **L1正则化**

   L1正则化通过约束参数的L1范数（L1-norm）减小过拟合。

   ![image-20240526113712217](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20240526113712217.png)

#### 常用的归一化和标准化方法：

1. 归一化是利用特征的最大值，最小值，将特征的值缩放到[0,1]区间，对于每一列的特征使用min - max函数进行缩放。

2.  归一化可以消除纲量，加快收敛。不同特征往往具有不同的量纲单位，这样的情况会影响到数据分析的结果，为了消除指标之间的量纲影响，需要进行数据归一化处理，以解决数据指标之间的可比性。原始数据经过数据归一化处理后，各指标处于[0,1]之间的小数，适合进行综合对比评价。

3.  归一化可能模型提高精度。

#### 常用的参数初始化方法：

![img](https://pic2.zhimg.com/80/v2-30d4d43f1ca7a6066a771ea80f6ea2a1_720w.webp)



#### 评价指标

- 分类问题
  - 准确率（Accuracy）
  - 召回率（Recall）
  - 精确率（Precision）
  - P-R曲线
  - F1-Score：F值是精确率和召回率的调和值，更接近于两个数较小的那个，所以精确率和召回率接近时，F值最大。很多推荐系统的评测指标就是用F值的。$F1 = 2*（1/Precision + 1/Recall）$
  - ROC.
  - AUC
  - 混淆矩阵
- 回归问题
  - 平均绝对误差（MAE）
  - 均方误差（MSE）
  - 归一化均方根误差（NRMSE）
  - 均方根误差（RMSE）
  - 决定系数（R2）

​		  

